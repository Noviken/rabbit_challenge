{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6758bdd-e4ad-452e-81a5-0395f635d6a6",
   "metadata": {},
   "source": [
    "### kNN,k-meanss　レポート\n",
    "\n",
    "- #### kNN（NN:NearestNeighbor)\n",
    "    - 分類問題のための機械学習法（教師あり）\n",
    "    - 最近棒のデータをk個とってきて、そのうち最も多く存在していたクラスとして識別する\n",
    "        - 当然ながら、kがいくつかによって、識別されるクラスは変動し得る\n",
    "        - kが大きい値であれば、決定境界は滑らかになる\n",
    "            - ただし、大きすぎると誤判定される領域が増えると思われる。チューニングが必要\n",
    "            - また、小さすぎるとこれまでに出てきた過学習のように、１データに適合し過ぎる\n",
    "            - もし同数だったらどうするのか？2値分類であれば、kは奇数で設定した方がよさそう\n",
    "\n",
    "- #### k-means\n",
    "    - kNNと名称は似ているが、こちらは教師なし学習であり、クラスタリングの手法\n",
    "    - 与えられたデータを、k個のクラスタに分割する\n",
    "    - アルゴリズムの概要\n",
    "        1. 各クラスタ中心となる初期値を設定\n",
    "        1. 各データ点にて各クラスタ中心との距離を計算。最も距離が近いクラスタを割り当てる\n",
    "        1. 各クラスタの平均ベクトル(中心)を計算する→中心を更新する\n",
    "        1. ２および3を収束するまで繰り返す\n",
    "    - 初期値によってクラスタリング結果が変わる。初期値同士が近いとうまくクラスタリングできない。\n",
    "        - k-means++法を用いると、なるべく離れた初期値を利用することになるため、この問題に対応できるらしい\n",
    "    - また（当然ながら）kが変われば、クラスタリングする数が変わることになるため、結果も変わる\n",
    "    - うまくクラスタリングできているのか、最適なパラメタはどのように決めればよいのか？(特に高次元データは可視化できず困難そう）\n",
    "        - 同じクラスタ数同士の結果比較には、WCSS（クラスタ内平方和）を比較すれば良い模様\n",
    "        - クラスタ数の決定において、目安としてElbow法で判断できなくもないらしい\n",
    "            - クラスタ数とWCSSでグラフを描画した際に、屈曲する（WCSSの減少が緩やかになる）点をクラスタ数とする\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d241605-2b60-4106-8924-c16331a7df8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
