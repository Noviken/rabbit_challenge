{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6758bdd-e4ad-452e-81a5-0395f635d6a6",
   "metadata": {},
   "source": [
    "### 線形回帰　レポート\n",
    "\n",
    "- 線形とは→ざっくりいうと、比例の意.  \n",
    "$$\n",
    "\\begin{align}\n",
    "    f(x_1,x_2,...,x_{n-1}) &= a_0 + a_1x_1 + a_2x_2 + ... + a_{n-1}x_{n-1} \\\\\n",
    "    &= \\sum_{i=0}^{n-1}(a_ix_i) \\quad (x_0=1) \\\\\n",
    "    &=\\boldsymbol{a}^T・\\boldsymbol{x}   \n",
    "\\end{align} $$\n",
    "($\\boldsymbol{a},\\boldsymbol{x}$はn次元の列ベクトル) \n",
    "\n",
    "- 回帰問題\n",
    "    - ある入力（離散あるいは連続値）から出力（連続値）を予測する問題\n",
    "        - 直線で予測→線形回帰\n",
    "        - 曲線で予測→非線形回帰\n",
    "- 【参考】バプニック(Vapnik)の原理\n",
    "    - ランキング問題を回帰で解くのはおすすめしない  \n",
    "      （本来やりたいことをするのに、そのためにそれより難しいことをするべきではない）\n",
    "- 回帰で扱うデータ\n",
    "    - 入力（各要素を、説明変数または特徴量と呼ぶ）\n",
    "        - m次元のベクトル（m=1の場合はスカラ）\n",
    "    - 出力（目的変数）\n",
    "        - スカラー値（目的変数）\n",
    "- 線形回帰モデル\n",
    "    - 回帰問題を解くための機械学習モデルの１つ\n",
    "    - 教師あり学習\n",
    "    - 入力とm次元パラメタの線形結合を出力.慣例として、予測値にはハットをつける\n",
    "    - パラメタを $ \\boldsymbol{w}=(w_1,w_2,...,w_m)^T $ とすると、予測値は  \n",
    "       $ \\qquad \\hat{y} = \\boldsymbol{w}^T・\\boldsymbol{x} + w_0 $\n",
    "    - 説明変数が多次元の場合。。。\n",
    "        - 線形重回帰モデルという　（次元１なら単回帰）\n",
    "        - 単回帰は直線だが、重回帰は曲面\n",
    "        - データへの仮定…データは回帰曲面に誤差が加わり観測されている \n",
    "            $ y = w_0 + w_1x_1 + w_2x_2 + \\epsilon $\n",
    "    - 線形回帰モデルのパラメタは、最小二乗法で推定\n",
    "        - 平均二乗誤差（残差平方和）… MSE\n",
    "            - データとモデル出力の二乗誤差の和の平均→小さいほど直線とデータとの距離近い\n",
    "            - MSE $  = \\frac{1}{n_{train}} \\sum_{i=1}^{n_{train}}\n",
    "                                      (\\hat{y}_{i}^{(train)} - y_{i}^{(train)})^2 $\n",
    "        - 最小二乗法\n",
    "            - 学習データの平均二乗誤差を最小とするパラメタを探索\n",
    "            - 学習データの平均二乗誤差の最小化は、その勾配が0となる点を求めれば良し \n",
    "                $$\n",
    "                \\begin{align}   \n",
    "                  \\frac{\\partial}{\\partial{w}}(\\rm{MSE}_{train}) &= 0 \\\\\n",
    "                  \\Rightarrow  \\frac{\\partial}{\\partial{w}}[\n",
    "                      \\frac{1}{n_{train}}(Xw - y)^T(Xw - y)] &= 0 \\\\\n",
    "                  \\Rightarrow  \\frac{\\partial}{\\partial{w}}[\n",
    "                      \\frac{1}{n_{train}}(w^TX^T - y^T)(Xw - y)] &= 0 \\\\\n",
    "                  \\Rightarrow  \\frac{1}{n_{train}} \\frac{\\partial}{\\partial{w}}[\n",
    "                      (w^TX^TXw -2w^TX^Ty + y^Ty)] &= 0 \\\\\n",
    "                  \\Rightarrow  \\frac{1}{n_{train}} [\n",
    "                      (2X^TXw -2X^Ty)] &= 0 \\\\\n",
    "                  \\Rightarrow  \\hat{w} &= (X^Tw)^{-1} X^Ty \\\\\n",
    "                \\end{align}\n",
    "                $$ \n",
    "            \n",
    "- データの分割とモデルの汎化性能測定\n",
    "    - データの分割\n",
    "        - 学習用 : モデルの学習に利用\n",
    "        - 検証用 : モデルの精度検証に利用\n",
    "    - なぜ分割する？\n",
    "        - モデルの”汎化性能”を測定するため\n",
    "        - 我々の目的は、既知データに対する当てはまりの良さではなく \n",
    "          未知のデータに対して精度良く予測すること\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6c9508-0209-4035-81d5-ea80bff9e542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
