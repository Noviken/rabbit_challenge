{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6758bdd-e4ad-452e-81a5-0395f635d6a6",
   "metadata": {},
   "source": [
    "### 深層学習（day2)　レポート\n",
    "\n",
    "- #### Section1 勾配消失問題について\n",
    "    - 勾配消失問題\n",
    "        - 誤差逆伝搬法が下位層に進んでいくにつれ、勾配がどんどん緩やかになっていく。そのため勾配降下法による更新では下位層のパラメタがほとんど変わらず、訓練が最適値に収束しなくなる。\n",
    "    - 解決法\n",
    "        - 活性化関数の選択\n",
    "            - ReLU : 勾配消失問題の回避とスパース化に貢献\n",
    "        - 重みの初期値設定\n",
    "            - Xavier　：　重みの要素を、前の層のノード数の平方根で除算した値に\n",
    "                - 利用する場合の活性化関数：ReLU,sigmoid,tanh\n",
    "            - He : 重みの要素を、前の層のノード数の平方根で除算した値に対しsqrt(2)をかけ合わせた値\n",
    "                - 利用する場合の活性化関数:ReLU\n",
    "        - バッチ正規化\n",
    "            - ミニバッチ単位で、入力値のデータの偏りを抑制する手法\n",
    "            - 活性化関数に値を渡す前後に、バッチ正規化の処理を孕んだ層を加える\n",
    "        \n",
    "- #### Section2 学習率最適化手法について\n",
    "    - 学習率の復習\n",
    "        - 値が大きい場合、最適値にいつまでのたどり着かず発散してしまう\n",
    "        - 値が小さい場合、発散することはないが収束するまでに時間がかかってしまう。また大域局所最適値に収束し辛くなる\n",
    "    - 初期の学習率設定方法の指針\n",
    "        - 初期の学習率を大きく設定し、徐々に小さくしていく\n",
    "        - パラメタ毎に学習率を可変させる\n",
    "    - 学習率最適化手法\n",
    "        - momentum\n",
    "            - 誤差をパラメタで微分したものと学習率の積を減算した後、現在の主にに前回の重みを減算した値と慣性の積を加算\n",
    "            - 局所最適にならず、大域的最適解となる。谷間に着いてから最適値にいくまでの時間が早い\n",
    "        - AdaGrad\n",
    "            - 誤差をパラメタで微分したものと再定義した学習率の積を減算する\n",
    "            - 勾配の緩やかな斜面に対して、最適値に近づける\n",
    "            - （課題）学習率が徐々に小さくなるので、鞍点問題を引き起こすことがあった\n",
    "        - RMSProp\n",
    "            - 誤差をパラメタで微分したものと再定義した学習率の積を減算する\n",
    "            - 局所最適にならず、大域的最適解となる。ハイパラの調整が必要な場合が少ない\n",
    "        - Adam\n",
    "            - momentumとRMSPropのメリットを孕んだアルゴリズム\n",
    "            - momentumの過去の勾配の指数関数的減衰平均と、RMSPropの過去の勾配の2乗の指数関数的減衰平均を孕む\n",
    "\n",
    "- #### Section3 過学習について\n",
    "    - 過学習の復習\n",
    "        - テスト誤差と訓練誤差とで学習曲線が乖離すること\n",
    "    - 正則化\n",
    "        - networkの自由度（層数、ノード数、パラメタ値など）を成約すること\n",
    "    - 荷重減衰\n",
    "\n",
    "    - 正則化手法\n",
    "        - 荷重減衰\n",
    "            - 重みが大きい値をとることで、過学習が発生することがある\n",
    "            - 誤差に対して正則化項を加算することで重みを抑制\n",
    "            - L1正則化、L2正則化\n",
    "                - 誤差関数に、p-ノルムを加える（p=1の場合L1正則化、2の場合L2正則化と呼ぶ）\n",
    "        - ドロップアウト\n",
    "            - ランダムにノードを削除して学習させること\n",
    "            - データ量を変化させずに、異なるモデルを学習させていると解釈できる（アンサンブル学習的な？）\n",
    "    \n",
    "- #### Section4 畳み込みニューラルネットワークの概念\n",
    "    - CNN（Convolutional Neural Network)\n",
    "        - 画像処理でよく用いられるNN。畳み込み層やプーリング層で構成される\n",
    "    - 畳み込み層\n",
    "        - 画像の場合、縦横チャネルの３次元のデータをそのまま学習し、次に伝えることができる\n",
    "        - (結論)３次元の空間情報も学習できるような層が畳み込み層\n",
    "    - プーリング層\n",
    "        - 切り出した層の代表値を取得する\n",
    "        - Max pooling（最大値を代表値とする）やAvg pooling(平均値を代表値とする）などがある\n",
    "    \n",
    "- #### Section５ 最新のCNN\n",
    "    - AlexNet\n",
    "        - 2012年にHinton教授らによって発表された物体認識のためのモデルであり、物体認識のために初めて深層学習、CNNの概念を取り入れたとされる。\n",
    "        - ５層の畳み込み層、及びプーリング層等、それに続く３層の全結合層から構成される。\n",
    "        - 過学習を防ぐ施策として、サイズ4096の全結合層の出力にドロップアウトを使用\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d241605-2b60-4106-8924-c16331a7df8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
