情報理論　レポート

■自己情報量
  I(x) = -log(P(x)) = log(W(x))
 　・対数の底が2のとき、単位はbit
 　・対数の底がeのとき、単位はnat
　 ・対数の底が10のとき、単位はdigit
　 ・稀な事象の方が、情報量が大きい
　 ・情報量は加法生を持つ
　　　（独立な事象A,Bに対して、AもBも起こる確率は
　　　　Aの情報量とBの情報量の和となる）

■シャノンエントロピー
　H(x) = E( I(x) )
       = -E( log(P(x)) )
       = -Σ{ P(x)log(P(x)) }
　　・自己情報量の期待値
　　・平均情報量や情報量のエントロピーともいう

■カルバック・ライブラー　ダイバージェンス
　D_KL(P||Q) = E_P[ log(P(x)/Q(x)) ] = E_P[ logP(x) -logQ(x) ]
　　・同じ事象・確率変数における、異なる確率分布PとQの差異を表す
　　・カルバック・ライブラー情報量や、情報利得、カルバック・ライブラー距離とも。

■交差エントロピー
 QのPに対する交差エントロピー
　　H(P,Q) = -E_p[ log(Q(x)) ]
　　　　　　= H(P) + D_KL(P||Q)
　　・Qについての自己情報量を、Pの分布で平均化したもの
　　・クロスエントロピーともいう
　　・機械学習においては、損失関数として使われたりする。
　　　例えば分類問題において、真の分布ｐ（＝教師ラベル）に対して、
　　　推定結果ｑがどの程度誤差があるかを表す尺度として。


以上
