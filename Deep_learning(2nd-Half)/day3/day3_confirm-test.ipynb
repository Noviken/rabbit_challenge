{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6758bdd-e4ad-452e-81a5-0395f635d6a6",
   "metadata": {},
   "source": [
    "### 深層学習 day3 確認テスト\n",
    "\n",
    "- #### Section0.深層学習全体像の復習-1\n",
    "    - サイズ5*5の入力画像を3*3フィルタで畳み込んだときの出力画像のサイズを答えよ。ストライドは2、パディングは1とする\n",
    "        - 3×3\n",
    "\n",
    "- #### Section1.再帰型ニューラルネットワークについて-1\n",
    "    - RNNにおける重みは大きく分けて３つあり、1つは入力から現在の中間層を定義する際にかけられる重み、１つは中間層から出力を定義する際にかけられる重みである。残り１つの重みについて説明せよ。\n",
    "        - 過去の中間層から現在の中間層を定義する際にかけられる重み\n",
    "\n",
    "- #### Section1.再帰型ニューラルネットワークについて-２\n",
    "    - 連鎖律の原理を用い、dz/dxを求めよ\n",
    "        - dz/dx = dz/dt　×　dt/dx = 2t = 2(x+y)　\n",
    "\n",
    "- #### Section1.再帰型ニューラルネットワークについて-3\n",
    "    - 図のy1を数式で表わせ。バイアスは任意の文字で定義せよ。また、中間層の出力にシグモイドg(x)を作用させよ。\n",
    "        - $ y_1 = sigmoid(W_{(out)}z_1 + c)$\n",
    "        - $ z_1 = sigmoid(W_{(in)}x_1 + Wz_0 + b)$\n",
    "\n",
    "\n",
    "- #### Section2.LSTM-1\n",
    "    - 以下の文章をLSTMに入力し空欄に当てはまる単語を予測したいとする。文中の「とても」という言葉は空欄の予測においてなくなっても影響を及ぼさないと考えられる。このような場合、どのゲートが作用すると考えられるか。『映画おもしろかったね。ところで、とてもお腹が空いたから何か＿＿＿＿』\n",
    "        - 忘却ゲート\n",
    "\n",
    "- #### Section3.GRU-1\n",
    "    - LSTMとCECが抱える問題について、それぞれ簡潔に述べよ\n",
    "        - LSTM:パラメタが多く、したがって計算負荷が高い\n",
    "        - CEC:学習能力がない（そのため、ゲートがある\n",
    "\n",
    "- #### Section3.GRU-２\n",
    "    - LSTMとGRUの違いを簡潔に述べよ\n",
    "        - LSTMは、CECと入力・出力・忘却ゲートで構成されるが、GRUは更新ゲートとリセットゲートで構成\n",
    "        - GRUの方がパラメタ数が小さくなり、したがって計算負荷も低くなる\n",
    "\n",
    "- #### Section5.Seq2Seq-1\n",
    "    - 選択肢の中から、Seq2Seqについて説明しているものを選べ\n",
    "        - (2) RNNを用いたEncoder-Decoderモデルの一種であり、機械翻訳などのモデルに使われる\n",
    "\n",
    "- #### Section5.Seq2Seq-２\n",
    "    - Seq2SeqとHRED、VHREDの違いを簡潔に述べよ\n",
    "        - Seq2Seqは会話の流れ（文脈）を無視した応答を行ってしまう課題があったが、HREDはそれを考慮できるようになった。\n",
    "        - しかしながら、HREDは確率的な多様性が字面にしかなく、したがって会話の流れに多様性はない（毎回会話の流れが同じになる）また、短く情報量に乏しい回答をしがち。これらを解決したのがVHRED\n",
    "\n",
    "- #### Section5.Seq2Seq-3\n",
    "    - VAEに関する下記の説明文中の空欄に当てはまる言葉を答えよ\n",
    "        - 自己符号化器の潜在変数に　確率分布　を導入したもの\n",
    "        \n",
    "- #### Section7.AttentionMechanism-1\n",
    "    - RNNとWord2vec、Seq2SeqとAttentionの違いを簡潔に述べよ\n",
    "        - ＲＮＮは時系列データを処理するのに適したNNモデルであり、可変長の文字列を与えることができない（すなわち、固定長形式で単語を与える）必要があった。ｗｏｒｄ２ｖｅｃにより入力(単語)の（固定長）分散表現ベクトルを得ることが可能になった。ｓｅｑ２ｓｅｑは系列データを入力とし別の時系列データを出力するEncoder-Decoder構造のNWであり、入力系列が長い場合にDecoder側に入力情報を確り伝えることが困難であったが、それを解決するための機構がＡｔｔｅｎｔｉｏｎ。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a73bc9-6eb8-4624-b9d4-982b52245085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
