{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6758bdd-e4ad-452e-81a5-0395f635d6a6",
   "metadata": {},
   "source": [
    "### 深層学習（day3)　レポート\n",
    "\n",
    "- #### Section1 再帰型ニューラルネットワークの概念\n",
    "    - RNNとは\n",
    "        - （時）系列データに対応可能なニューラルネットワーク\n",
    "    - 時系列データとは\n",
    "        - 時間的順序を追って一定間隔ごとに観察され、しかも相互の統計的依存関係が認められるようなデータの系列\n",
    "        - 具体的な時系列データとは　→　音声データ、テキストデータ、etc\n",
    "    - RNNの特徴\n",
    "        - 時系列モデルを扱うには、初期の状態と過去の時間t-1の状態を保持し、そこから次の時間でのtを再帰的に求める再帰構造が必要になる\n",
    "    - BPTT\n",
    "        - Back Propagation Through Time\n",
    "        - RNNにおけるパラメタ調整方法の一種であり、誤差逆伝搬法の一種。誤差が時間を遡り伝搬する\n",
    "        \n",
    "- #### Section2 LSTM\n",
    "    - RNNの課題\n",
    "        - 時系列を遡れば遡るほど、勾配が消失していく → 長い時系列の学習が困難\n",
    "        - 解決策として、ネットワークの構造自体を変えて解決したものがLSTM\n",
    "    - 勾配消失問題の復習\n",
    "        - 下位層に進んでいくにつれて勾配がどんどん緩やかになり、勾配降下法によるパラメタ更新はほとんど変わらずに、訓練が最適値に収束しなくなる\n",
    "    - 勾配爆発\n",
    "        - 勾配が層を逆伝搬するごとに、指数関数的に大きくなっていく\n",
    "    - LSTM\n",
    "        - CEC\n",
    "            - 勾配消失および爆発の解決方法として、勾配が1であれば解決できる\n",
    "            - CECの課題：入力データについて時間依存度に関係なく重みが一律である。ということは、NNの学習特性がないということ。。\n",
    "        - 入力ゲート・出力ゲート\n",
    "            - 入力ゲート・出力ゲートを追加することで、それぞれのゲーへの入力値の重みを、重み行列W,Uで可変可能とする\n",
    "            - → CECの課題の解決\n",
    "        - LSTMブロックの課題と忘却ゲート\n",
    "            - CECは過去の情報が全て保管されているが、過去の情報が要らなくなった場合に削除することができない\n",
    "            - この課題の解決のために、忘却ゲートを導入\n",
    "        - 覗き穴結合\n",
    "            - CECに保管されている過去の情報を、任意のタイミングで他のノードへ伝搬させたり、あるいは任意のタイミングで忘却させたい。CEC自身の値はゲート制御に影響を与えていない。\n",
    "            - のぞき穴結合：CEC自身の値に、重み行列を介して伝搬可能にした構造\n",
    "    \n",
    "- #### Section3 GRU\n",
    "    - GRU(Gated Recurrent Unit))\n",
    "    - 従来のLSTMではパラメタが多数存在していたため計算負荷大。GRUではそのパラメタを大幅に削減し、かつ精度は同等またはそれ以上が望めるようになった構造、\n",
    "    \n",
    "- #### Section4 双方向RNN\n",
    "    - 過去の情報だけでなく、未来の情報をも加味することで、精度を向上させるためのモデル\n",
    "    - 文章の推敲や、機械翻訳などの実用例がある（未来までの情報が必要となるため、利用方法は制限される）\n",
    "    \n",
    "- #### Section５ Seq2Seq\n",
    "    - Encoder - Decoder モデルの一種\n",
    "    - 機械対話や機械翻訳などに利用されている\n",
    "    - Encoder RNN\n",
    "        - ユーザがinputしたテキストデータを、単語などのトークンに区切って渡す構造\n",
    "            - Taking:文章を単語等のトークン毎に分割し、トークンごとのIDに分割する\n",
    "            - Embedding:IDから、そのトークンを表す分散表現ベクトルに変換\n",
    "            - Encoder RNN:ベクトルを順番にRNNに入力していく\n",
    "        　- 処理手順\n",
    "             - vec1をRNNに入力し、hidden stateを出力。このhidden stateと次の入力vec2をまたRNNに入力してきたhidden stateを出力という流れを繰り返す\n",
    "             - 最後のvecを入れたときのhidden stateをfinal stateとしてとっておく。このfinal stateがthought vectorと呼ばれ、入力した文の意味を表すベクトルとなる\n",
    "    - Decoder RNN\n",
    "        - システムがアウトプットデータを、単語等のトークンごとに生成する構造\n",
    "        - 処理手順\n",
    "            1. Decoder RNN:Encoder RNNのfinal state(thought vector)から、各tokenの生成確率を出力していくfinal stateをDecoder RNNのinitial stateとして設定し、Embeddingを入力\n",
    "            1. Sampling:生成確率にもとづいて、tokenをランダムに選択\n",
    "            1. Embedding:Samplingで選ばれたtokenをEmbeddingし、Decoder RNNへの次の入力とする\n",
    "            1. Detokenize:1〜3を繰り返し、2で得られたtokenを文字列に直す\n",
    "    - HRED\n",
    "        - seq2seqの課題：1問１答しかできず、文脈を考慮することができない\n",
    "        - HRED:過去n-1個の発話から、次の発話を生成する（甘えの単語の流れに即して応答されるため、より人間らしい文章の生成が可能）\n",
    "        - HRED: Seq2Seq + Context RNN\n",
    "            - Context RNN : Encoderのまとめた各文章の系列をまとめて、これまでの会話コンテキスト全体を表すベクトルに変換する構造→過去の発話の履歴を加味した返答をできる\n",
    "        - HREDの課題\n",
    "            - HREDは確率的な多様性が字面にしかなく、会話の「流れ」のような多様性がない\n",
    "                - 同じコンテキスト（発話リスト）を与えられても、答えの内容が毎回会話の流れとしては同じものしかできない\n",
    "            - HREDは短く情報量に乏しい答えをしがち\n",
    "                - 短いよくある答え（「うん」「そうだね」「・・・」など）を学ぶ傾向がある            \n",
    "    - VHRED\n",
    "        - VHREDとは？\n",
    "            - HREDにVAEの潜在変数の概念を追加したもの\n",
    "            - それにより、上述したHREDの課題を解決した構造\n",
    "    - VAE\n",
    "        - オートエンコーダー\n",
    "            -　教師なし学習の１つ。そのため学習時の入力データは訓練データのみで、教師データは利用しない\n",
    "            -　具体例：MNISTの場合、28×28の数字の画像を入れて、同じ画像を出力するNN\n",
    "            -　構造：入力データから潜在変数zに変換するNNをEncoder、逆に潜在変数zをインプットとして元画像を復元するNNをDecoder\n",
    "            - メリット：次元削減が行える（zの次元が入力データのそれより小さい場合、次元削減とみなせる）\n",
    "        - VAE\n",
    "            - 通常のオートエンコーダーは、何かしら潜在変数zにデータを押し込めているが、その構造がどのような状態かわからない\n",
    "            - VAEはこの潜在変数zに確率分布z-N(0,1)を仮定したもの。データを潜在変数zの確率分布という構造に押し込めることを可能にする、\n",
    "\n",
    "- #### Section6 word2vec\n",
    "    - 課題：RNNでは、単語のような可変長の文字列をNNに与えることはできない。→固定長形式で単語を表す必要がある\n",
    "    - word2vecは、単語の分散表現ベクトルの獲得を目的とした手法\n",
    "        - 学習データからボキャブラリを作成\n",
    "        - One-hotベクトル\n",
    "        - メリット：大規模データの分散表現の学習が、現実的な計算速度とメモリ量で実現可能になった\n",
    "            - ×：ボキャブラリ×ボキャブラリだけの重み行列が誕生\n",
    "            - ○：ボキャブラリ×任意の単語ベクトル次元で重み行列が誕生\n",
    "\n",
    "- #### Section7 Attention Mechanism\n",
    "    - 課題：Seq2Seqでは、長い文章への対応が難しい。単語数が2でも100でも、固定次元ベクトルの中に入力しなければならない。\n",
    "    - 解決策：文章が長くなるほどそのシーケンスの内部表現の次元も大きくなっていく仕組み（”入力と出力のどの単語が関連しているのか”の関連度を学習する仕組み）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d241605-2b60-4106-8924-c16331a7df8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
